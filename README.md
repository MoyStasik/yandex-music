# Яндекс музыка

## 1. Тема и целевая аудитория

### Тема

**Яндекс музыка** - музыкальный стриминговый сервис компании Яндекс, позволяющий слушать музыкальные композиции, их подборки, альбомы и получать персональные рекомендации. 

### Целевая аудитория
+ Количество уникальных пользователей за месяц: 24млн [2] *
+ Количество посещений приложения в месяц: 81.6 миллионов [1]
+ Среднее количество страниц за посещение: 2.58 [1]
+ Среднее количество посещений одного пользователя за месяц: 21 день
+ Среднестатистический пользователь слушает около 20-30 треков в день
+ 28% пользователей слушают подкасты хотя бы раз в месяц [3]

*- Данные за декабрь 2024 года


**Веб-трафик по странам**

![image](https://github.com/user-attachments/assets/28474299-8247-4df0-a4bc-13c64783e4a7)
Рисунок 1 - веб-трафик по странам
**Демографические показатели посещаемости сайта**

![image](https://github.com/user-attachments/assets/cad65256-a375-4c5f-811c-ef3f374f86f8)
Рисунок 2 - демографические показатели посещения сайта

### Ключевой функционал
+ Регистрация/авторизация
+ Воспроизведение музыки
+ Добавление треков в плейлист, удаление из плейлиста, хранение плейлистов
+ Поиск исполнителя, трека, альбома
+ История прослушивания треков
+ Поиск музыки по жанру
+ прослушивание подкастов

### Ключевые продуктовые решения
+ Личные рекомендации под каждого пользователя
+ Выбор музыкального настроения, для воспроизведения соответствующей музыки

### Список используемых источников
1. [https://www.similarweb.com/ru/website/music.yandex.ru](https://www.similarweb.com/ru/website/music.yandex.ru)
2. [https://www.rbc.ru/technology_and_media/02/12/2024/674db2029a79474943d5748e](https://www.rbc.ru/technology_and_media/02/12/2024/674db2029a79474943d5748e)
3. [https://vc.ru/media/187539-auditoriya-podkastov-na-yandeksmuzyke-vyrosla-v-chetyre-raza-za-god-hotya-by-raz-v-mesyac-ih-slushayut-uzhe-28-podpischikov](https://vc.ru/media/187539-auditoriya-podkastov-na-yandeksmuzyke-vyrosla-v-chetyre-raza-za-god-hotya-by-raz-v-mesyac-ih-slushayut-uzhe-28-podpischikov)


## 2. Расчет нагрузки
### Продуктовые метрики
Таблица 1 - Количество активных пользователей
<table>
    <tr>
        <th>Период</th>
        <th>Количество пользователей</th>
    </tr>
    <tr>
        <td>День</td>
        <td>тк среднее количество посещений для пользователя за месяц это 21 день, то в среднем DAU = 21/30 * 24 = 16,8млн</td>
    </tr>
    <tr>
        <td>Месяц</td>
        <td>24 миллиона</td>
    </tr>
</table>

Таблица 2 - Средний размер хранилища пользователя
<table>
    <tr>
        <th>Тип хранилища</th>
        <th>Средний объем на 1 пользователя</th>
    </tr>
    <tr>
        <td>Плейлисты</td>
        <td>Проанализируем данные о количестве плейлистов в приложениях аналогах - спотифае, по данным википедии в спотифае около 2млрд плейлистов[6], учитывая, что в спотифае около 600млн уникальных пользователей за месяц, то можем грубо оценить, что на человека приходится 4 плейлиста, для нашего приложения возьмем такую же цифру, тк привычки пользователей похожи. Сколько треков в среднем на плейлист информация нигде не приводится, поэтому был проведен анализ среди знакомых, дальше будут приведена сумма треков в плелистах каждого из опрошенных: (200 + 250 + 354 + 81 + 1090 + 342 + 100 + 1038 + 34) / 9 = 387 треков в среднем пользователи добавляют в плейлисты, на 1 плейлист приходится около 387 / 4 = 97 треков, итого 1 плейлист состоит из обложки - размер фото около 30 КБ(данные из девтулз), название плейлиста - 30 байт, хранение одной песни в виде id и другой метаинформации занимает около 0.1Кб (данные из devtools). Итого: 4*(30Кбайт + 30 байт + 0,1Кбайт) + 387*0,1Кбайт = 0,12МБ + 0,04МБ = 0,16МБ </td>
    </tr>
  <tr>
        <td>История прослушиваний</td>
        <td>Данных о количество уникальных треков, которые слушает пользователь за год в публичном доступе нет, но тк есть информация что в день среднестатистический пользователь слушает около 30 разных треков и в месяц пользователь проводит на сайте 21 из 30 дней, то можем посчитать по верхней границе, если 80 процентов треков, которые пользователь слушает не повторяются, то за год пользователь в среднем слушает 30 * 21 * 12 * 0,8 = 6048 уникальных треков. Для хранения истории необходимо 0.1Кбайт метаифнормации для 1 трека. Итого:  6048 * 0,1Кбайт = 0,6МБ</td>
    </tr>
</table>

Таблица 3 - Действия пользователей по типам
<table>
    <tr>
        <th>Тип действия</th>
        <th>Среднее количество в день</th>
    </tr>
    <tr>
        <td>Воспроизведение музыки</td>
        <td>Среднестатистический пользователь прослушивает в день около 30 треков, так как всего в сутки 16,8 млн пользователей, то всего в день прослушивается 16,8 * 30 = 504 млн</td>
    </tr>
  <tr>
        <td>Авторизация</td>
        <td>Возьмем, что авторизационный токен живет 1 месяц, тогда каждому пользователю нужно будет проходить авторизацию 1 раз в месяц, тогда кол-во авторизаций в сутки: 24 млн / 30 = 800 тыс</td>
    </tr>
    <tr>
        <td>Регистрация</td>
        <td> Месячный рост новых пользователей составляет около 1,5%, тогда в месяц новых пользователей 24млн * 0,015 = 360 000, в день получается новых пользователей: 360 000/30 = 12 000</td>
    </tr>
    <tr>
        <td>Добавление/удаление треков в/из плейлист</td>
        <td> 1,554 млн*</td>
    </tr>
    <tr>
        <td>Прослушивание треков из истории</td>
        <td> В среднем пользователь 1 раз включает трек из истории прослушиваний, всего в сутки 16,8млн юзеров, тогда всего обращений к истории треков - 16,8 * 1 = 16,8 млн </td>
    </tr>
    <tr>
        <td>Поиск исполнителя, трека, альбома</td>
        <td> Среднее количество поисков за день - 57,6 млн*</td>
    </tr>
    <tr>
        <td>Поиск треков по жанру</td>
        <td> 140 тыс*</td>
    </tr>
    <tr>
        <td>Прослушивание подкастов</td>
        <td>в месяц 28% пользователей слушают подкасты хотя бы раз[2], из этих данных можем узнать, что 24*0,28/30 = 224 000 в неделю хотя бы раз слушают подкасты</td>
    </tr>
</table>
* - данные, придуманные автором.

### Технические метрики
Битрейт песен возьмем - 128 Кбит/c.[3]

В среднем размер треков составляет 3 минуты 17 секунд[7], тогда с битрейтом 128Кбит/c размер одного трека = (197с × 128Кбит/с) / 8 / 1024 = 3,06Мб и обложка/картинка 20КБайт, итого 3,08МБ.[1]

В среднем подкасты длятся от 30 до 60минут[8], возьмем среднюю длительность 45 минут, подкасты как и треки будем хранить с битрейтом 128Кбит/c, тогда они будут занимать около 3,06МБ * 15 = 46МБ памяти + 20КБ на обложку, итого 46,02МБ на 1 подкаст.

Таблица 4 - Размер хранения в разбивке по типам данных
<table>
    <tr>
        <th>Тип хранения</th>
        <th>Размер</th>
    </tr>
    <tr>
        <td>Пользователи</td>
        <td>из таблицы 2, на одного пользователя необходимо (0,16 + 0,6) = 0,76 МБ, тогда на 24 млн пользователей 17,4ТБ</td>
    </tr>
    <tr>
        <td>Треки</td>
        <td>на 1 трек 3,08 МБ, всего 76 млнов треков, тогда для хранения всех понадобится 223,2ТБ[5]</td>
    </tr>
    <tr>
        <td>Подкасты</td>
        <td>В Яндекс музыке около 25,9тыс подкастов, тогда все подкасты будут занимать около 1,12ТБ[4]</td>
    </tr>
</table>

Таблица 5 - Сетевые метрики
<table>
    <tr>
        <th>Тип действия</th>
        <th>Пиковое потребление в течение суток, Гбит/с</th>
        <th>Суммарный суточный, Гбайт/сутки</th>
        <th>RPS (средний)</th>
        <th>RPS (пиковый)</th>
    </tr>
    <tr>
        <td>Воспроизведение музыки</td>
        <td>140 000 * 128Кбит/с = 17 920 000Кбит/c = 17,1</td>
        <td>16,8млн * 60мин * 60с * 128 Кбит/c / 2^23 = 922 852</td>
        <td>Всего 16,8 млн юзеров, тк в сутки примерно 60 минут средний пользователь слушает музыку, и запрос на новый чанк случается раз в ~ 10 секунд, те ~ 6 раз в минуту, то средний рпс будет, rps = 16,8млн * 60мин * 6 / 86400с = 70 000</td>
        <td>В пик около 140тыс пользователей слушает музыку одновременно, rps = 140 000 </td>
    </tr>
    <tr>
        <td>Авторизация</td>
        <td>14 * 16Кбайт / 2^20 = 0,0002</td>
        <td>Сетевой трафик одного запроса - 16Кбайт, тогда 800тыс * 16Кбайт / 2^20 = 12,2</td>
        <td>Всего 24млн месячных пользователей, среднестатистический пользователь заходит в музыку 21 день из 30, авторизационный токен необходимо обновлять 1 раз в месяц, тогда средний рпс = 24млн / 30 / 86400 = 9,3</td>
        <td>Пиковый рпс на авторизацию = 14</td>
    </tr>
    <tr>
        <td>Регистрация</td>
        <td>0,20 * 16Кбайт / 2^20 = 0,0000032</td>
        <td>Всего 12 000 * 16Кбайт / 2^20 = 0,18</td>
        <td>Месячный рост новых пользователей составляет около 1,5%, тогда в месяц новых пользователей 24млн * 0,015 = 360 000, в день получается 12 000 новых пользователей, тогда средний рпс будет = 12 000 / 86400 = 0,14</td>
        <td>Пик рпс = 0,20</td>
    </tr>
    <tr>
        <td>Добавление/удаление треков в/из плейлист</td>
        <td>Пиковый rps = 30 * 1,2 Кбайт / 2^20 = 0,00003</td>
        <td>Размер одного запроса - 1,2Кбайт (информация из dev tools) => всего 1,554млн * 1,2Кбайт / 2^20 = 1,78</td>
        <td>Средний рпс = 1,554млн / 86400 = 18</td>
        <td>Пиковый рпс = 30</td>
    </tr>
    <tr>
        <td>Получение треков из истории</td>
        <td>291 * 15,5Кбайт / 2^20 = 0,004</td>
        <td>16,8млн * 1 * 15,5Кбайт (информация из dev tools) / 2^20 = 248,3</td>
        <td>rps = 16,8млн * 1 / 86400 = 194</td>
        <td>Пиковый rps = 291</td>
    </tr>
    <tr>
        <td>Поиск исполнителя, трека, альбома</td>
        <td>1000 * 15Кбайт / 2^20 = 0,014</td>
        <td>57,6млн * 15Кбайт (информация из dev tools) / 2^20 = 824</td>
        <td>rps = 57,6млн / 86400 = 667</td>
        <td>Пиковый rps = 667 * 1,5 = 1000</td>
    </tr>
    <tr>
        <td>Поиск треков по жанру</td>
        <td>2,5 * 210Кбайт / 2^20 = 0,0005</td>
        <td>140тыс * 210Кбайт (информация из dev tools) / 2^20 = 28</td>
        <td>rps = 140тыс / 86400 = 1,62</td>
        <td>Пиковый rps = 2,5</td>
    </tr>
    <tr>
        <td>Прослушивание подкастов</td>
        <td>1 866 * 128Кбит/с / 2^20 = 0,23</td>
        <td>224 000 * 60мин(средняя длительность прослушивания подкастов)[2] * 60с * 128Кбит/c / 2^23 = 12 305</td>
        <td>rps = 224 000 * 60мин(средняя длительность прослушивания подкастов)[2] * 6 / 86400 = 933</td>
        <td>Пиковый рпс = 933 * 2 = 1 866</td>
    </tr>
</table>

### Список используемых источников
1. [https://yandex.ru/company/news/03-21-08-2024](https://yandex.ru/company/news/03-21-08-2024)
2. [https://vc.ru/media/187539-auditoriya-podkastov-na-yandeksmuzyke-vyrosla-v-chetyre-raza-za-god-hotya-by-raz-v-mesyac-ih-slushayut-uzhe-28-podpischikov](https://vc.ru/media/187539-auditoriya-podkastov-na-yandeksmuzyke-vyrosla-v-chetyre-raza-za-god-hotya-by-raz-v-mesyac-ih-slushayut-uzhe-28-podpischikov)
3. [https://www.amplifier.ru/news/2024/06/12/yandeks-muzyka-110624.html#:~:text=Пользователям%20будут%20доступны%20три%20опции,без%20потерь%20в%20формате%20Lossless.](https://www.amplifier.ru/news/2024/06/12/yandeks-muzyka-110624.html#:~:text=Пользователям%20будут%20доступны%20три%20опции,без%20потерь%20в%20формате%20Lossless.)
4. [https://yandex.ru/company/researches/2021/podcasts#:~:text=В%20каталоге%20Яндекс.Музыки%2011,запускалось%20более%20500%20новых%20проектов.](https://yandex.ru/company/researches/2021/podcasts#:~:text=В%20каталоге%20Яндекс.Музыки%2011,запускалось%20более%20500%20новых%20проектов.)
5. [https://ru.wikipedia.org/wiki/Яндекс_Музыка#:~:text=По%20состоянию%20на%202023%20год,пользуется%20около%2020%20млн%20человек.](https://ru.wikipedia.org/wiki/Яндекс_Музыка#:~:text=По%20состоянию%20на%202023%20год,пользуется%20около%2020%20млн%20человек.)
6. [https://ru.wikipedia.org/wiki/Spotify#:~:text=В%20настоящее%20время%20в%20«Спотифай,как%20пользователями%2C%20так%20и%20кураторами.](https://ru.wikipedia.org/wiki/Spotify#:~:text=В%20настоящее%20время%20в%20«Спотифай,как%20пользователями%2C%20так%20и%20кураторами.)
7. [https://daily.afisha.ru/news/58899-srednyaya-dlina-pesen-s-godami-sokraschaetsya/#](https://daily.afisha.ru/news/58899-srednyaya-dlina-pesen-s-godami-sokraschaetsya/#)
8. [https://roistat.com/rublog/podkast/#:~:text=В%20среднем%20подкасты%20длятся%20от%2030%20минут%20до%201%20часа.](https://roistat.com/rublog/podkast/#:~:text=В%20среднем%20подкасты%20длятся%20от%2030%20минут%20до%201%20часа.)

## 3. Глобальная балансировка нагрузки
### Функциональное разбиение по доменам
Тк сервис связан с большим количеством загружаемого контента, то я решил разбить все на 2 домена - первый будет служить, как раздача статического контента: музыки, подкастов, обложек, а второй домен будет для апишки:
+ cdn-music.yandex.ru - данный домен будет использоваться для раздачи музыки, подкастов, обложек
+ music.yandex.ru - данный домент будет служить для действий с апи - авторизация/регистрация, добавление/удаление в/из плейлиста и др. 

### Расположение ДЦ
Учитывая веб трафик пользователей яндекс музыки на рисунке 3 и учитывая магистральные сети связи в России[1], можно выделить следующие города с ДЦ:
+ Москва
+ Санкт-Петербург
+ Краснодар
+ Казань
+ Новосибирск
+ Хабаровск
![image](https://github.com/user-attachments/assets/28474299-8247-4df0-a4bc-13c64783e4a7)
Рисунок 3 - веб трафик по странам

**Пояснение:**

В Москву и Санкт-Петербург можно поставить cdn, чтобы раздавали статику, суммарный средний рпс на музыку, подкасты = 70000 + 993 = 70993, предлагаю поставить 3 ДЦ в Москву и 3 ДЦ в Санкт-Петербург, что поможет в отказоустойчивости, если один ДЦ упадет, другие смогут выдержать входящий трафик, также количество ДЦ обусловлено большой нагрузкой на получение статического контента.
Остальные ДЦ будут не такими нагруженными и будет достаточно по 1 ДЦ в каждом городе, даже если ДЦ в ближайшем городе выйдет из строя, другой регион сможет выдержать нагрузку

Тк мало пользователей приходится на Нидерланды и Беларусь, то нет смысла для них выделять отдельные ДЦ, поэтому трафик в этих странах будет идти в Москву. По той же причине нет смысла ставить ДЦ в Казахстан/Узбекистан их трафик будет направляться в Новосибирск/Казань

### Расчет распределение запросов из секции "Расчет нагрузки" по типам запросов по датацентрам
Для расчета распределения запросов воспользуемся картой плотности России.
![image](https://github.com/user-attachments/assets/c85603c8-1b6f-4a40-acf3-6b10065e61ca)



+ cdn-music.yandex.ru - наибольшая нагрузка будет от запросов на этот домен, тк он будет отвечать за раздачу статики на клиенты
+ music.yandex.ru - запросы на этом домене будут отвечать за запросы к апи

запросы к апи требуют в среднем (9,3 + 0,14 + 18 + 194 + 667 + 1,62) = 890 рпс из этого можно построить таблицу 6.

Тк около 70 процентов населения России расположены в европейской части, то сделаем следующее распределение: средний рпс * 0,7 поделим на 4 города Москва, Санкт-Петербург, Краснодар, Казань. В Мск и Питере больше всего ДЦ и потенциальная будет испытываться наибольшая нагрузка на эти ДЦ, можем равномерно распределить нагрузку на каждый отдельный ДЦ. Всего на европейскую часть 3 + 3 + 1 + 1 = 8 ДЦ, тогда каждый отдельный ДЦ будет обрабатывать около 70% / 8 = 8,75% запросов пользователей.

Казань также будет захватывать часть трафика Зауралья(Екатеринбург, Уфа и близлижайшие), поэтому на этот ДЦ будет чуть больше нагрузки, возьмем 13,75% запросов.

Оставшиеся 25% Азиатской части будут покрывать ДЦ в Новсибирске и Хабаровске, для них сделаем равную нагруженность 25% / 2 = 12,5% запросов пользователей.

Таблица 6 - Распределение запросов на домен music.yandex.ru
<table>
    <tr>
        <th>Датацентры</th>
        <th>Средний рпс</th>
    </tr>
    <tr>
        <td>Москва на 3 ДЦ</td>
        <td>890(всего средний рпс) * ( 0,0875 * 3(ДЦ) ) = 233,6</td>
    </tr>
    <tr>
        <td>Санкт-Петербург на 3 ДЦ</td>
        <td>890(всего средний рпс) * ( 0,0875 * 3(ДЦ) ) = 233,6</td>
    </tr>
    <tr>
        <td>Краснодар</td>
        <td>890(всего средний рпс) * ( 0,0875 * 1(ДЦ) ) = 77,9</td>
    </tr>
    <tr>
        <td>Казань</td>
        <td>890(всего средний рпс) * ( 0,1375 * 1(ДЦ) ) = 122,375</td>
    </tr>
    <tr>
        <td>Новосибирск</td>
        <td>890(всего средний рпс) * ( 0,1225 * 1(ДЦ) ) = 109</td>
    </tr>
    <tr>
        <td>Хабаровск</td>
        <td>890(всего средний рпс) * ( 0,1225 * 1(ДЦ) ) = 109</td>
    </tr>
</table>

Воспроизведение музыки и подкастов требует (70 000 + 993) = 70 993 запроса в секунду, из этого можно построить таблицу 7.

Таблица 7 - Распределение запросов на домен cdn-music.yandex.ru
<table>
    <tr>
        <th>Датацентры</th>
        <th>Средний рпс</th>
    </tr>
    <tr>
        <td>Москва на 1 ДЦ</td>
        <td>70 993 / 6 = 11 832</td>
    </tr>
    <tr>
        <td>Санкт-Петербург на 1 ДЦ</td>
        <td>70 993 / 6 = 11 832</td>
    </tr>
</table>

### Схема DNS балансировки
Тк ДЦ географически удалены друг от друга, будет логично использовать Geo-based DNS. Однако из-за того, что почти все ДЦ расположены в России могут возникать проблемы с резолвингом. Поэтому в данном случае решил воспользоваться Latency-based DNS, тк при загрузке музыки необходима скорость, обеспечивающаяся за счет наименьшего latency. Этот вид балансировки будет использоваться для запросов к апи, т.е. при обращении к домену music.yandex.ru

### Схема Anycast балансировки
Тк в Москве и Санкт-Петербурге будет находиться статика, будет разумно для балансировки трафика использовать BGP Anycast, объединив ДЦ в Москве и Питере под одним ip. Этот вид балансировки будет использоваться для запросов к cdn, т.е. при обращении к домену cdn-music.yandex.ru 

### Список используемых источников
1. [https://www.comnews.ru/content/230456/2023-12-20/2023-w51/1180/magistralnye-seti-svyazi-rossii-2023#map-section](https://www.comnews.ru/content/230456/2023-12-20/2023-w51/1180/magistralnye-seti-svyazi-rossii-2023#map-section)


## 4. Локальная балансировка нагрузки
### Cхемы балансировки для входящих и внутренних запросов
+ Буду использовать Kubernetes кластеры, тк они подходят и для балансировки запросов, и для оркестрации;
+ Для балансировки внешних запросов будет использоваться Nginx, как L7 балансировщик;
+ До Nginx трафик будет балансироваться с помощью LVS via Direct Routing, для этого хосты должны быть в одной физической сетке;
+ Nginx работает как HTTP Reverse Proxy балансировщик на уровне L7.

### Схема отказоустойчивости
+ Для обеспечения отказоустойчивости будет использоваться k8s и Auto-scaling. Тем самым Service discovery будет реализован за счёт оркестрации. Система оркестрации при создании контейнера вносит экземпляр в реестр после успешного прохождение readiness пробы. Так же с помощью readiness-проб обеспечивается исключение из кластера упавших подов; 
+ Использование Keepalived;
+ К тому же, периодически может проводиться проверка работоспособности сервера посредством отсылания на него базовых L7 health checks;
+ Следим за нагрузкой балансировщиков, макс нагрузка 80 процентов, если будет больше, то будем увеличивать их количество, если какая то железка умрет, то трафик перераспределиться по остальным, за счет того, что будет запас по нагрузке на каждом балансировщике, будет время восстановить/купить новую железку
### Нагрузка по терминации SSL
В среднем SSL handshake занимает 3ms[1]. Мы будем использовать session cache. Необходимо рассчитать, какое количество запросов будут кешироваться:
(70000 + 194 + 667 + 1,62 + 11200) / (70000 + 9,3 + 0,14 + 18 + 194 + 667 + 1,62 + 11200) = 99 % - запросов попадает в кеш.

Суммарный пиковый рпс = (140000 + 14 + 0,20 + 30 + 291 + 1000 + 2,5 + 22400) = 163 737, тогда выходит, что 163 737 * (1 - 0,99) = 1 637 запросов не попадают в кеш, на обработку ssl уйдет 1637 * 3ms = 4,9s процессорного времени
### Использование источников
1. [https://www.ibm.com/docs/en/cics-ts/6.x?topic=performance-ssl-handshake-overhead](https://www.ibm.com/docs/en/cics-ts/6.x?topic=performance-ssl-handshake-overhead)


## 5. Логическая схема БД

### Таблица, поля и связи между ними

На рисунке ниже представлена диаграмма отношений между выделеннными мной таблицами.

<img width="1217" alt="image" src="https://github.com/user-attachments/assets/e7698a16-b817-4025-b77d-906989f87fc0" />


Ниже представлено описание всех таблиц.

<br>
Таблица 8 - Описание выделенных таблиц
<table>
    <tr>
        <th>Имя таблицы</th>
        <th>Поля таблицы</th>
        <th>Краткое описание таблицы</th>
    </tr>
    <tr>
        <td>Session</td>
        <td>id - уникальный хеш сессии<br>
            user_id - id пользователя, которому принадлежит сессия<br>
            expires - дата, когда истечет сессия<br>
            created_at - дата, когда сессия была создана<br>
            updated_at - дата обновления записи</td>
        <td>В таблице хранятся сессии пользователей</td>
    </tr>
    <tr>
        <td>Users</td>
        <td>id - уникальный id пользователя<br>
        username - никнейм пользователя<br>
        password - зашифрованный пароль пользователя<br>
        img_path - путь на аватарку пользователя, хранящуюся в s3<br>
        created_at - дата создания пользователя<br>
        updated_at - дата обновления записи</td>
        <td>В данной таблице хранится инофрмация о пользователях</td>
    </tr>
    <tr>
        <td>Authors</td>
        <td>id - уникальный id автора<br>
        nickname - никнейм автора<br>
        img_path - путь на фото автора, хранящееся в s3<br>
        created_at - дата создания записи об авторе<br>
        updated_at - дата обновления записи</td>
        <td>Данная таблица хранит информацию об исполнителях</td>
    </tr>
    <tr>
        <td>Genres</td>
        <td>id - уникальный id жанра<br>
        genre_name - наименование жанра</td>
        <td>Таблица описывает музыкальные жанры</td>
    </tr>
    <tr>
        <td>Songs</td>
        <td>id - уникальный id песни<br>
        author_id - id автора, которому принадлежит песня<br>
        genre_id - id жанра, к которому принадлежит песня<br>
        song_file_path - путь до файла с песней, хранящегося в s3<br>
        created_at - дата создания записи о песни<br>
        updated_at - дата обновления записи</td>
        <td>Данная таблица описывает сущность песни</td>
    </tr>
    <tr>
        <td>Playlists</td>
        <td>id - уникальный id плейлиста<br>
        user_id - id пользователя, которому принадлежит плейлист<br>
        song_id - id песни, которая находится в плейлисте<br>
        img_path - путь до файла с картинкой плейлиста, хранящегося в s3<br>
        playlist_name - название плейлиста<br>
        created_at - дата создания плейлиста<br>
        updated_at - дата обновления записи</td>
        <td>Таблица хранит информацию о плейлистах</td>
    </tr>
    <tr>
        <td>History</td>
        <td>id - уникальный id истории<br>
        user_id - id пользователя, которому принадлежит история<br>
        song_id - id песни, которая находится в истории<br>
        started_at - timestamp создания записи об истории прослушивания<br>
        updated_at - дата обновления истории(когда закончил прослушивание)</td>
        <td>Таблица хранит историю прослушивания треков каждого пользователя</td>
    </tr>
    <tr>
        <td>Thematics</td>
        <td>id - уникальный id тематики для подкаста<br>
        thematic_name - наименование тематики</td>
        <td>Таблица описывает тематики подкастов</td>
    </tr>
    <tr>
        <td>Podcasts</td>
        <td>id - уникальный id плейлиста<br>
        author_id - id пользователя, которому принадлежит плейлист<br>
        thematic_id - id песни, которая находится в плейлисте<br>
        podcast_name - путь до файла с картинкой плейлиста, хранящегося в s3<br>
        description - название плейлиста<br>
        podcast_file_path - путь до файла с подкастом, хранящегося в s3<br>    
        created_at - дата создания плейлиста<br>
        updated_at - дата обновления записи</td>
        <td>Таблица хранит информацию о подкастах</td>
    </tr>
</table>

### Размеры данных и нагрузки на чтение/запись

Ниже представлена оценочная информация о данных для каждой таблицы.

<br>
Таблица 9 - Размеры данных каждой таблицы
<table>
    <tr>
        <th>Название таблицы</th>
        <th>Размер данных таблицы</th>
    </tr>
    <tr>
        <td>Session</td>
        <td>Допустим, длина хеша - 40 символов. Тогда размер данных: 24 млн * (40байт + 8байт + 8байт + 8байт) = 1,43Гб</td>
    </tr>
    <tr>
        <td>Users</td>
        <td>Для id пользователей будем использовать id размеров 8 байт, username пользователя в среднем состоит из 10 символов, захешированный пароль из 32 символов, пусть путь до аватарки пользователя состоит из 25 символов, дата создания занимает 8 байт. Итого 24млн * (8байт + 10байт + 32байт + 25байт + 8байт) = 1,86Гб</td>
    </tr>
    <tr>
        <td>Authors</td>
        <td>по данным спотифая, на платформе около 11 млн артистов[1], для Яндекс музыки возьмем 10 млн артистов, 10млн * (8 + 10 + 25 + 8)байт = 0,47Гбайт</td>
    </tr>
    <tr>
        <td>Genres</td>
        <td>Данные по количеству жанров представленных в Яндекс музыки я не нашел, однако в статистике по спотифаю написано, что их представлено более 5000[2], возьмем это значение для Яндекс музыки. Среднее количество символов для жанра - 8. Итого 5000 * (8 + 8)байт = 0,08Мб</td>
    </tr>
    <tr>
        <td>Songs</td>
        <td>Раннее мы выяснили что в Яндекс музыки представлено около 76млн треков, длина пути до файла в среднем будет занимать около 25 символов. Итого размер хранилища: 76млн * (8 + 8 + 8 + 25 + 8)байт = 4,03Гбайт</td>
    </tr>
    <tr>
        <td>Playlists</td>
        <td>Раннее мы выяснили, что на человека приходится около 4 плейлистов, тогда всего около 24млн * 4 = 96млн плейлистов, длина пути до картинки занимает около 25 символов, средняя длина названия плейлиста - 10 символов. Посчитаем размер хранилища: 96млн * (8 + 8 + 8 + 25 + 10 + 8)байт = 5,28Гбайт</td>
    </tr>
    <tr>
        <td>History</td>
        <td>Раннее мы выяснили, что около 6048 треков хранятся в истории прослушиваний, тогда будет 6048 разных song_id, рассчитаем размер хранилища: 24млн * (8 + 8 + 6048*8 + 8 + 8)байт = 1,06Тб</td>
    </tr>
    <tr>
        <td>Thematics</td>
        <td>Посчитаем размер таблицы, при 30 разных жанров[3] и при 10 символов в среднем на тематику. 30 * (8 + 10) байт = 0,001Мбайт</td>
    </tr>
    <tr>
        <td>Podcasts</td>
        <td>Раннее мы расчитали, что всего в Яндекс музыки представлено около 25,9тыс подкастов, название подкастов в среднем занимает около 15 символов, описание подкаста в среднем состоит из 40 слов - 320 символов. 25,9тыс * (8 + 8 + 8 + 15 + 40 + 25 + 8) = 0,003Гб</td>
    </tr>
    <tr>
        <td>Итого</td>
        <td>1,073Тб</td>
    </tr>
</table>

Оценим еще размер хранилищ с файлами треков и подкастов.

Таблица 10 - Размеры хранилищ
<table>
    <tr>
        <th>Тип хранения</th>
        <th>Размер</th>
    </tr>
    <tr>
        <td>Треки</td>
        <td>223,2ТБ</td>
    </tr>
    <tr>
        <td>Подкасты</td>
        <td>1,12ТБ</td>
    </tr>
</table>

Расчет данных взят из таблицы 4.

Посчитаем нагрузку на чтение и запись, смотреть таблицу 11.

Таблица 11 - Нагрузка на чтение и на запись
<table>
    <tr>
        <th>Название таблицы</th>
        <th>Нагрузка на чтение</th>
        <th>Нагрузка на запись</th>
    </tr>
    <tr>
        <td>Session</td>
        <td>Сессия проверяется при каждом запросе - рпс = 82 090(средний суммарный рпс)</td>
        <td>В месяц 24млн * 2 = 48млн(создать/удалить). rps = 18,5</td>
    </tr>
    <tr>
        <td>Users</td>
        <td>Чтение пользователя необходимо после авторизации, тк будут загружаться плейлисты, рекомендации для пользователя, то примерно будет происходить 16,8млн чтений(как и авторизация), тогда рпс = 194</td>
        <td>В месяц создается около 360 000 новых аккаунтов, в день 12 000, рпс = 0,14</td>
    </tr>
    <tr>
        <td>Authors</td>
        <td>Чтение автора происходит при прослушивании треков и поиске исполнителя, прослушивание треков 504млн в сутки, поиск исполнителя, трека, альбома 57,6млн в месяц, во всех этих запросах нужен будет автор, тогда 504 + 57,6 = 581,6млн, рпс = 6 731</td>
        <td>По статистике спотифай за 3 года количество исполнителей увеличилось на 3 млна[4], тогда в год 1 млн новых исполнителей, а в день тогда около 2 740 новых исполнителей, возьмем такую же статистику для Яндекс музыки, тогда рпс = 0,032</td>
    </tr>
    <tr>
        <td>Genres</td>
        <td>Чтение жанров происходит при прослушивание музыки и поиске, прослушивание треков 504млн в сутки, поиск исполнителя, трека, альбома 57,6млн в месяц, во всех этих запросах нужен будет жанр, тогда 504 + 57,6 = 581,6млн, рпс = 6 731</td>
        <td>Количество жанров в Яндекс музыки в рамках записей в бд небольшое, тк всего суммарно 5000 жанров, то рпс будет незначительным</td>
    </tr>
    <tr>
        <td>Songs</td>
        <td>В сутки слушается около 504млн треков, тогда рпс = 5 833</td>
        <td>По статистике в спотифай выходит около 49000 новых песен в сутки[6], тогда рпс на запись = 0,57</td>
    </tr>
    <tr>
        <td>Playlists</td>
        <td>Плейлисты загружаются, когда пользователь заходит в аккаунт, те 16,8млн раз, тогда рпс = 194</td>
        <td>Из раннее полученных данных, мы выяснили, что удаление/добавление в плейлист происходит 1,554млн раз в сутки, тогда рпс = 18</td>
    </tr>
    <tr>
        <td>History</td>
        <td>Раннее мы выяснили, что в сутки пользователь совершает 1 запрос для получения истории прослушиваний, тогда чтение будет 16,8млн в сутки, рпс = 194</td>
        <td>История будет записываться при каждом прослушивании треков, всего в сутки прослушивается 504 млна треков, тогда рпс = 5 833</td>
    </tr>
    <tr>
        <td>Thematics</td>
        <td>Чтение тематик происходит при прослушивании подкастов, не получилось найти сколько подкастов слушает конкретный человек в сутки, есть только информация что в среднем слушатель подкастов проводит 60 минут за прослушиванием, а в среднем длительность подкаста 45минут, тогда возьмем, что в среднем, один слушатель подкаста в сутки открывает 2 подкаста, тогда 224 000 * 2 = 448 000 подкастов слушается в сутки, тогда рпс = 5,2</td>
        <td>Количество тематик в Яндекс музыки как и жанров ограниченно, поэтому рпс будет незначительным</td>
    </tr>
    <tr>
        <td>Podcasts</td>
        <td>Как мы выяснили выше 448 000 подкастов слушается каждый день, тогда рпс = 5,2</td>
        <td>Около 450 подкастов выходит в день[5], тогда рпс = 0,005</td>
    </tr>
</table>

Ниже представлены нагрузки на хранилища.

Таблица 11 - Нагрузка на чтение и на запись
<table>
    <tr>
        <th>Название таблицы</th>
        <th>Нагрузка на чтение</th>
        <th>Нагрузка на запись</th>
    </tr>
    <tr>
        <td>Треки</td>
        <td>Из раннее полученных данных рпс на музыку будет 70 000рпс, битрейт 128Кбит/c тогда нагрузка на чтение 70 000 * 128 = 8 960 000 Кбит/с = 8,54Гбит/c</td>
        <td>Нагрузка на запись треков - рпс = 0,57, Нагрузка = 0,57 * 3,06МБ(размер одного трека). Итого нагрузка на запись = 1,74Мб/с</td>
    </tr>
    <tr>
        <td>Подкасты</td>
        <td>Из раннее полученных данных рпс на подкасты будет 933рпс, битрейт 128Кбит/c тогда нагрузка на чтение 933 * 128Кбит/c = 0,11Гбит/с</td>
        <td>Нагрузка на запись подкастов - рпс = 0,005 * 46,02МБ(размер одного подкаста). Итого нагрузка на запись = 0,23Мб/с</td>
    </tr>
</table>

### Указать требования к консистентности

Рассмотрим условия консистентности внтури таблиц.

Таблица 12 - Условия консистентности внутри таблиц
<table>
    <tr>
        <th>Название таблицы</th>
        <th>Условия консистентности</th>
    </tr>
    <tr>
        <td>Session</td>
        <td>Сессия уникальна<br>
Пользователь уникален</td>
    </tr>
    <tr>
        <td>Users</td>
        <td>У каждого пользователя уникальный ID</td>
    </tr>
    <tr>
        <td>Authors</td>
        <td>У каждого автора уникальный ID
        </td>
    </tr>
    <tr>
        <td>Genres</td>
        <td>У каждого жанра уникальный ID<br> У каждого жанра уникальное название</td>
    </tr>
    <tr>
        <td>Songs</td>
        <td>У каждой песни уникальный ID <br> Файл песни должен существовать в хранилище</td>
    </tr>
    <tr>
        <td>Playlists</td>
        <td>У каждого плейлиста должен быть уникальный ID<br></td>
    </tr>
    <tr>
        <td>History</td>
        <td>У каждой истории уникальный ID<br> user_id уникален У каждой истории существует user_id <br> user_id должен быть единствинен для каждого history.id</td>
    </tr>
    <tr>
        <td>Thematics</td>
        <td>У каждой тематики уникальный ID<br> У каждой тематики уникальное название</td>
    </tr>
    <tr>
        <td>Podcasts</td>
        <td>У каждого подкаста уникальный ID <br> Файл подкаста существует в хранилище</td>
    </tr>
</table>

Далее рассмотрим условия консистентности между таблицами.

Таблица 13 - Условия консистентности между таблицами

<table>
    <tr>
        <th>Название таблицы</th>
        <th>Условия консистентности</th>
    </tr>
    <tr>
        <td>Songs</td>
        <td>Должен существовать genre_id<br> Существует author_id</td>
    </tr>
    <tr>
        <td>Playlists</td>
        <td>Существует user_id</td>
    </tr>
    <tr>
        <td>History</td>
        <td>user_id существует и уникален</td>
    </tr>
    <tr>
        <td>Podcasts</td>
        <td>Существует thematic_id</td>
    </tr>
</table>

### Указать особенности распределения нагрузки по ключам
Можно выделить следующие особенности:

+ Очень большая нагрузка приходится на чтение таблицы songs, так как она является одной из главных: песни фигурирует в большинстве функционала. Нагрузка на запись будет не такой большой. Ключ: song_id;
+ Большая нагрузка приходится на чтение таблицы genres, тк она нужна для поиска песен, в прослушивании треков, также как и в таблице songs, основная часть будет приходиться на чтение. Ключ: genre_id;
+ Таблица authors будет использоваться похожим образом с жанрами, основная нагрузка будет на чтение. Ключ: author_id.
+ Таблица history также будет часто использоваться, тк запись в нее будет при прослушивании каждого трека. Основная нагрузка будет на запись. Ключ: history_id.
+ Большая нагрузка на чтение будет для таблицы session, тк сессия будет проверяться при каждом запросе. Основная нагрузка - чтение. Ключ: session_id.

### Используемые источники

1. https://www-searchlogistics-com.translate.goog/learn/statistics/spotify-statistics/?_x_tr_sl=en&_x_tr_tl=ru&_x_tr_hl=ru&_x_tr_pto=rq#:~:text=Key%20Spotify%20Statistics,-Despite%20not%20being&text=Spotify%20has%2082%20million%20songs,creators%20on%20the%20Spotify%20platform
2. https://dtf.ru/music/668422-kak-naiti-svoi-v-5000-podkapotnyh-zhanrov-spotify
3. https://translate.google.com/translate?u=https://voice123.com/blog/marketing-trends-tactics/types-of-podcasts/&hl=ru&sl=en&tl=ru&client=rq&prev=search#:~:text=There%20are%20about%2030%20different,fictional%20or%20non%2Dfictional%20narration.
4. https://xmldatafeed.com/spotify-stats-2022-fakty-dannye-polzovateli-ispolzovanie-i-chasy-proslushivaniya/amp/
5. https://habr.com/ru/amp/publications/591545/
6. https://translate.google.com/translate?u=https://www.billboard.com/pro/how-much-music-added-spotify-streaming-services-daily/&hl=ru&sl=en&tl=ru&client=rq&prev=search#:~:text=Spotify%20finished%202022%20with%20more,49%2C000%20new%20songs%20per%20day.


## 6. Физическая схема БД

### Таблица, поля и связи между ними

На рисунке ниже представлена представлены таиблцы физической схемы БД и связи между ними.

<img width="1166" alt="image" src="https://github.com/user-attachments/assets/be773f69-e287-472f-b7cc-fec7efb727ed" />

Также весь медиа контента(песни, подкасты, изображения) будут храниться в s3 хранилище.

### Индексы

Далее рассмотрим индексы для таблиц

Таблица 14 - Индексы таблиц
<table>
    <tr>
        <th>Название таблицы</th>
        <th>Индексы таблицы</th>
    </tr>
    <tr>
        <td>users</td>
        <td>Поле id, чтобы искать конкретного пользователя</td>
    </tr>
    <tr>
        <td>history</td>
        <td>Поле user_id, чтобы искать историю конкретного пользователя</td>
    </tr>
    <tr>
        <td>playlists</td>
        <td>Поле user_id, чтобы искать плейлисты для конкретного пользователя</td>
    </tr>
    <tr>
        <td>media_filse</td>
        <td>Поле authors_names, чтобы искать песни конкретного автора<br>
        поле genres, чтобы искать песни по жанрам</td>
    </tr>
</table>

### Денормализация

Для сравнения предлагаем обратиться к рисунку выше с логической схемой БД. Также, в таблице 15 выделены выполненные для денормализации действия.

Таблица 15 - Денормализация таблиц
<table>
    <tr>
        <th>Название таблицы</th>
        <th>Выбранная СУБД</th>
    </tr>
    <tr>
        <td>users</td>
        <td>Был добавлен массив плейлистов, чтобы не приходилось хранить множественное кол-во отдельных записей с idшниками плейлистов</td>
    </tr>
    <tr>
        <td>history</td>
        <td>В таблицу history был добавлен массив с idшниками медиа файлов(треки/подкасты), чтобы не приходилось хранить множественное кол-во отдельных записей с idшниками треков</td>
    </tr>
    <tr>
        <td>playlists</td>
        <td>Был добавлен массив с idшниками треков</td>
    </tr>
    <tr>
        <td>media_files</td>
        <td>Был добавлен массив жанров, чтобы не приходилось делать JOIN с таблицей жанров<br>
        Было добавлено поле массив имен авторов и массив с ссылками на изображения авторов, это было сделано, также, чтобы избежать JOIN на таблицу авторов</td>
    </tr>
</table>

### Выбор СУБД

В таблице ниже прописан выбор СУБД и обоснование

Таблица 16 - Выбор СУБД для таблиц
<table>
    <tr>
        <th>Название таблицы</th>
        <th>Выбранная СУБД</th>
    </tr>
    <tr>
        <td>session</td>
        <td>Для хранения сессий был выбран Tarantool, тк он позволяет быстро читать данные, в данном случае скорость важна, хочу отметить, что, также, можно было использовать Redis, но Tarantool более надежен и чуть быстрее в записи и чтении [1]</td>
    </tr>
    <tr>
        <td>history</td>
        <td>Для истории прослушиваний был выбран ClickHouse, тк это OLAP СУБД, по истории будем собирать аналитику и формировать в будущем рекомендации</td>
    </tr>
    <tr>
        <td>Остальные таблицы</td>
        <td>Для таблиц users, playlists, media_files была выбрана Cassandra, тк она достаточно хорошо работает с высокими нагрузками запись/чтение, хорошо горизонтально масштабируется - с увеличением узлов в кластере линейно масштабируется</td>
    </tr>
</table>

Также для популярных треков, подкастов, треков, которые чаще всего слушает пользователь, предлагаю их кешировать с помощью Tarantool, те часть media files, которые находятся в Cassandra, будет дублироваться в Tarantool, тк key-value СУБД позволяет получить более быстрый доступ к данным. Для данного случая преимущества Tarantool над Redis будут такими же, как в представленном раннее обосновании такого выбора.

### Шардирование и резервирование СУБД (потаблично)

Шардирование таблиц будет сделано по следующим критериям:

+ Пользователей разделять по хешу id, отдельный шард на горячих пользователей(наиболее активные)
+ Медиа контент разделять по хешу id, отдельный шард на популярные треки
+ Плейлисты шардировать по хешу user_id 
+ таблицу session шардировать по хешу user_id
+ таблицу history шардировать по хешу user_id 

Резервирование СУБД будет реализовано через модель Master-Slave, так как нагрузка на чтение много превышает нагрузку на запись. К тому же, задержка в несколько секунд для консистентности не критична.

### Список используемых источников

1. https://habr.com/ru/companies/kts/articles/730518/#:~:text=Tarantool%20%D1%87%D1%83%D1%82%D1%8C%20%D0%B1%D1%8B%D1%81%D1%82%D1%80%D0%B5%D0%B5%20%D0%B2%20%D0%BE%D0%BF%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D1%8F%D1%85,%2C%20%D0%B0%20Redis%20%E2%80%94%20%D0%B2%20%D1%83%D0%B4%D0%B0%D0%BB%D0%B5%D0%BD%D0%B8%D0%B8.

## 7. Алгоритмы

**Лента треков**

Рекомендация треков будет происходить следующим образом:

+ Для подбора рекомендаций будет использоваться ML
+ В качестве БД для хранения метрик будет использоваться ClickHouse
+ Пересчитывать метрики будет сервис на python

Рекомендации будут формироваться на основе плейлистов пользователя. Изначально была идея учитывать историю прослушивания треков, но существует проблема, что у пользователя может на фоне долго играть какой-то трек или пользователь просто ищет новую музыку и слушает разное, но при этом оно ему не нравится. Поэтому было решено использовать плейлисты, тк пользователь сам добавил этот трек - значит он понравился. Рассчет будет таков, по трекам в плейлисте будет формироваться список жанров, которые слушает пользователь, те 10 треков были альтернативный рок, 20 хип-хоп и так далее, все жанры считаем и складываем, получаем итоговое число - 100 процентов по всем жанрам, дальше для каждого отдельного жанра будет считаться метрика, например итоговое число получилось 150, а хип-хоп 15 треков в плейлисте, тогда 15/150 = 10% доля на хип хоп, так считаем для каждого жанра и получаем процентаж для каждого жанра. Когда пользователь будет просматривать плейлист дня/мою волну, то будем делать выборку, когда будем загружать треки пользователю то в них будет, как в примере 10% треков на хип-хоп и так по каждому жанру, треки будут относиться к исполнителям, которых слушает пользователь + похожие исполнители - будет собираться статистика по пользователям, у которых похожие вкусы - совпадают исполнители в одинаковых жанрах и исполнители, которых они слушают будут рекомендоваться нашему пользователю. 

При добавлении пользователем нового трека в плейлист статистика будет пересчитываться, для конкретного жанра увеличивать количество и пересчитывать процентаж для каждого жанра.

**Добавление прослушиваний в историю**

Тк запись в историю происходит очень часто, каждый трек у каждого пользователя будет записываться в БД, то лучше всего будет суммарно собирать до 10к новых записей истории в буфере и потом только пушить в ClickHouse.


## 8. Технологии

Ниже приведена таблица, в который описаны используемые технологии, область их применения и обоснование.

Таблица 17 - Используемые технологии
<table>
    <tr>
        <th>Технология</th>
        <th>Область применения</th>
        <th>Мотивационная часть</th>
    </tr>
    <tr>
        <td>Golang</td>
        <td>Backend</td>
        <td>Go достаточно высоко воспроизведетельный язык<br>
        Go предоставляет хорошую многопоточную модель, содержит в себе горутины<br>
        Имеет множество библиотек и широкое коммьюнити</td>
    </tr>
    <tr>
        <td>React + ts + vite</td>
        <td>Клиентская часть</td>
        <td>Такой выбор обусловлен несколькими причинами:<br>
        безопасность - ts обеспечивает строгую типизацию и предотвращает непредвиденные ошибки программистов<br>
        кроссплатформенность - используя реакт можно писать как браузерные клиенты, так и десктоп, мобилку (react native, electron)<br>
        быстрота - в качестве сборщика модулей используется vite, тк он быстрее своих аналогов</td>
    </tr>
    <tr>
        <td>Nginx</td>
        <td>Проксирование трафика и SSL терминация</td>
        <td>С помощью nginx можно балансировать входящий трафик<br> Также он может кешировать запросы и сжимать статический контент<br> Nginx периодически может проверять доступность серверов(health checking)</td>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>Оркестрация</td>
        <td>k8s позволяет разворачивать сервера<br> Имеет auto-scaling при повышении нагрузки на сервера<br> Также обеспечивает отказоустойчивоть приложения</td>
    </tr>
    <tr>
        <td>Tarantool</td>
        <td>Хранение сессий и треков</td>
        <td>Использование tarantool обосновано тем, что он in-memory, что обеспечивает высокую скорость чтения, которая нам нужна для получения сессий пользователя и треков, также он довольно надежный и быстрее в записи, чем его аналог - Redis(подробнее в пункте 7)</td>
    </tr>
    <tr>
        <td>Clickhouse</td>
        <td>Хранение истории и аналитики</td>
        <td>Использование Clickhouse обусловлено тем, что это OLAP СУБД, он выдерживает очень большие нагрузки на запись и чтение</td>
    </tr>
    <tr>
        <td>Grafana + Prometheus</td>
        <td>Метрики</td>
        <td>Графана и прометеус будут использоваться для хранения и отображения метрик, что также позволит проводить аналитику как frontend, так и backend</td>
    </tr>
    <tr>
        <td>gRPC</td>
        <td>Протокол для передачи данных между микросервисами</td>
        <td>Используется для передачи данных между сервисами внутри приложения, также может обеспечивать связь между сервисами, написанными с помощью разных языков</td>
    </tr>
    <tr>
        <td>Kafka</td>
        <td>Брокер сообщений</td>
        <td>Будет хранить историю прослушиваний, которая впоследствии будет записываться в Clickhouse, выдерживает большую нагрузку</td>
    </tr>
</table>

## 9. Схема проекта

<img width="1026" alt="image" src="https://github.com/user-attachments/assets/1709e76d-a0d2-4b8a-8956-1c3eda293c29" />

## 10. Обеспечение надежности

**Резервирование**

Инфраструктурное

Глобальная балансировка нагрузки:

Размещение инфраструктуры в нескольких географически распределенных дата-центрах в России. Защита от региональных сбоев (стихийные бедствия, отключения электросетей).

Изолированные зоны доступности (AZ):

В каждом регионе — AZ с независимым питанием, охлаждением и сетевым подключением.

**Резервные системы:**

Множественные источники электропитания (ИБП, дизель-генераторы).

Несколько интернет-провайдеров и резервные сетевые каналы.

Резервное копирование:

Ежедневные бэкапы метаданных (плейлисты, пользователи) и аудиофайлов. Хранение копий в другом ДЦ.

**Уровень приложений**

Active/Active:

Основные сервисы (API, стриминг) развернуты в нескольких AZ. Трафик распределяется через глобальный балансировщик (Nginx/CDN).

Резервирование ресурсов:

Автоскейлинг пулов серверов с запасом CPU/RAM для пиковых нагрузок (вечерние часы, релизы альбомов).

Автоматическое переключение:

При падении мастера БД — мгновенный переход на реплику (MongoDB ReplicaSet).

**Управление сбоями**

Health Checks и сегментирование

Health Checks:

Nginx периодически проверяет статус сервисов (HTTP 200 на /health). Недоступные экземпляры исключаются из ротации.

**Сегментирование API:**

Критичные: стриминг, аутентификация — отдельные высокодоступные кластеры.

Не критичные: рекомендации.

**Graceful Shutdown/Degradation**

**Graceful Shutdown:**

Перед остановкой контейнер завершает обработку текущих запросов (например, загрузка трека).

**Graceful Degradation:**

При нехватке ресурсов: отключаются рекомендации (ML-модели), чат, сложные поисковые фильтры.

При падении поиска: возвращаются топ-100 треков по популярности.

При сбое транскодера: аудио сохраняется в S3 в исходном формате.

**Работа с данными**

**CQRS и асинхронная обработка**

**Write:** загрузка треков, обновление плейлистов — пишутся в MongoDB.

**Read:** прослушивание, поиск — отдельные реплики и кэш Tarantool.

Асинхронная статистика:

События (прослушивание, лайки) отправляются в Kafka. Сервис аналитики агрегирует данные в ClickHouse для дашбордов.

**Репликация БД**

MongoDB ReplicaSet:

Primary/Secondary + Arbiter для быстрого выбора нового мастера.

Heartbeat между узлами для мониторинга доступности.

Tarantool HA:

Репликация данных между регионами. Кэширование топ-треков и сессий пользователей.

**Наблюдаемость**

Мониторинг

Grafana/Prometheus:

Метрики: RPS, задержки, ошибки 5xx, использование CPU/RAM.

Алерты при превышении порогов (например, latency > 500 мс).

**Логирование:**

Логи в Elasticsearch (JSON-формат).

Trace Logging через Yandex SeeTrace: отслеживание цепочек запросов по request_id.

**Прочее**

**Идемпотентность:**

Уникальные idempotency_key для операций (например, добавление трека в плейлист).

**Тестирование:**

**Хаос-инжиниринг:** имитация падений AZ, отключение БД(раз в месяц).

**Rate Limits:**

Ограничение запросов на API (например, 1000 RPM на пользователя).

**CDN:**

Распределение аудиофайлов через CDN (Cloudflare) для снижения задержек.

**Восстановление**

**Kafka RecoveryPoint:**

Восстановление необработанных событий после сбоя (например, пропущенные прослушивания).

**Nginx как Circuit Breaker:**

При 30% ошибок в сервисе — временное отключение его из ротации.

## 11. Базовый расчёт аппаратных ресурсов

Определим сколько единиц CPU и RAM нужно на легкую, среднюю и тяжелую бизнес логику.

1. Легкая бизнес логика

    + CPU: 1 ядро на каждые 5000 пиковых RPS
    + RAM: 10 МБ на каждые 100 пиковых RPS

2. Средняя бизнес логика

   + CPU: 1 ядро на каждые 100 пиковых RPS
   + RAM: 100 МБ на каждые 100 пиковых RPS
  
3. Тяжелая бизнес логика

   + CPU: 1 ядро на каждые 10 пиковых RPS
   + RAM: 100 МБайт на каждые 10 пиковых RPS
  
Таблица 18 - Расчет CPU и RAM по микросервисам
<table>
    <tr>
        <th>Сервис</th>
        <th>Категория</th>
        <th>Пик RPS</th>
        <th>Ядра</th>
        <th>RAM</th>
    </tr>
    <tr>
        <th>Воспроизведение музыки/подкастов</th>
        <th>Тяжелая</th>
        <th>141866</th>
        <th>141866 / 10 = 14186,6</th>
        <th>141866 / 10 * 100МБ = 1418660 </th>
    </tr>
    <tr>
        <th>Авторизация/регистрация</th>
        <th>легкая</th>
        <th>14</th>
        <th>14 / 5000 = 1</th>
        <th>14 / 100 * 10 = 2МБ</th>
    </tr>
    <tr>
        <th>Добавление/удаление треков в/из плейлиста</th>
        <th>Средняя</th>
        <th>30</th>
        <th>30 / 100 = 1</th>
        <th>30 / 100 * 100 = 30Мб</th>
    </tr>
    <tr>
        <th>Получение треков из истории</th>
        <th>Средняя</th>
        <th>291</th>
        <th>291 / 100 = 3</th>
        <th>291 / 100 * 100 = 291 МБ</th>
    </tr>
    <tr>
        <th>Поиск</th>
        <th>Сложная</th>
        <th>1002,5</th>
        <th>1002,5 / 10 = 101</th>
        <th>1002,5 / 10 * 100МБ = 10025МБ</th>
    </tr>
</table>

Таблица 19 - БД сервисов и их ресурсы
<table>
    <tr>
        <th>Название БД</th>
        <th>Сервис</th>
        <th>Целевая пиковая нагрузка</th>
        <th>CPU</th>
        <th>RAM (GB)</th>
    </tr>
    <tr>
        <th>Tarantool</th>
        <th>Все сервисы</th>
        <th>143203</th>
        <th>143203 / 5000 = 29</th>
        <th>143203 / 100 * 10 = 14320,3МБ = 14Гб</th>
    </tr>
    <tr>
        <th>Mongodb</th>
        <th>Поиск, добавление/удаление из плейлиста, прослушивание треков и подкастов</th>
        <th>142899</th>
        <th>142899 / 100 = 1429</th>
        <th>142899 / 100 * 100 = 142899МБ = 140ГБ</th>
    </tr>
    <tr>
        <th>Clickhouse</th>
        <th>История прослушиваний</th>
        <th>291</th>
        <th>291 / 100 = 3</th>
        <th>291 / 100 * 100 = 291МБ = 0,29 ГБ</th>
    </tr>
</table>

Таблица 20 - Хостинг для своих сервисов
<table>
    <tr>
        <th>Название</th>
        <th>Хостинг</th>
        <th>Конфигурация</th>
        <th>Cores</th>
        <th>Cnt</th>
        <th>Покупка</th>
        <th>Аренда</th>
    </tr>
    <tr>
        <th>kubernode</th>
        <th>own</th>
        <th>2x6338/16x32GB/2xNVMe4T/2x25Gb/s</th>
        <th>64</th>
        <th>(29+1429+3)/64 * 1,15(резерв 15%) = 35</th>
        <th>4500 * 35 = 157 500</th>
        <th>-</th>
    </tr>
    <tr>
        <th>Tarantool</th>
        <th>own</th>
        <th>1xAMD7302P/2x32GB/1xNVMe256Gb/2x25Gb/s</th>
        <th>16</th>
        <th>29 / 16 * 1,15 = 3</th>
        <th>6000$</th>
        <th>-</th>
    </tr>
    <tr>
        <th>Mongodb</th>
        <th>own</th>
        <th>1xAMD7302P/2x32GB/1xNVMe256Gb/2x25Gb/s</th>
        <th>16</th>
        <th>1429 / 16 * 1,15 = 103</th>
        <th>206000$</th>
        <th>-</th>
    </tr>
    <tr>
        <th>Clickhouse</th>
        <th>own</th>
        <th>1xAMD7302P/2x32GB/1xNVMe256Gb/2x25Gb/s</th>
        <th>16</th>
        <th>2</th>
        <th>4000$</th>
        <th>-</th>
    </tr>
</table>

Так, суммарно будет поднято 35 k8s:

+ 2 на auth
+ 2 на плейлисты
+ 3 на историю
+ 4 на поиск
+ 24 на воспроизведение музыки и подкастов

Но, в каждом k8s будет лежать по 3 экземпляра соответствующего микросервиса для отказоустойчивости; следуя данному анализу, можно составить таблицу 21.

Таблица 21 - Сервисы в оркестрации
